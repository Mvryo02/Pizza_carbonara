{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":274.54014,"end_time":"2025-05-27T21:50:06.102469","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-27T21:45:31.562329","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4177c0b3","cell_type":"code","source":"!pip install torch_geometric","metadata":{"execution":{"iopub.status.busy":"2025-05-29T08:28:02.86555Z","iopub.execute_input":"2025-05-29T08:28:02.86592Z"},"id":"xSkgt1zf-raF","outputId":"59f4a52f-5eb4-41e5-9fba-07432989fe78","papermill":{"duration":200.886553,"end_time":"2025-05-27T21:48:54.983195","exception":false,"start_time":"2025-05-27T21:45:34.096642","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bef7bb0a350>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch-geometric/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"id":"a93b45be","cell_type":"code","source":"!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git","metadata":{"id":"5oR2D2Us-xSQ","outputId":"7086cadf-a7fe-4d75-f271-6339bee8164d","papermill":{"duration":32.342192,"end_time":"2025-05-27T21:49:27.329044","exception":false,"start_time":"2025-05-27T21:48:54.986852","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"20b4f928","cell_type":"code","source":"%cd hackaton/","metadata":{"id":"tEhfPly6-7UK","outputId":"3078ee06-6312-4fca-f5f9-888fa628c80a","papermill":{"duration":0.011496,"end_time":"2025-05-27T21:49:27.344221","exception":false,"start_time":"2025-05-27T21:49:27.332725","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"545d087e","cell_type":"code","source":"!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n","metadata":{"id":"PxBvwB0_6xI8","outputId":"5933387c-2cfb-474f-d842-f36a3e2d2a73","papermill":{"duration":32.713594,"end_time":"2025-05-27T21:50:00.061519","exception":false,"start_time":"2025-05-27T21:49:27.347925","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"3fd073a9","cell_type":"code","source":"!ls -lh datasets","metadata":{"id":"1rockhiQ7Nny","outputId":"2cd2e6f4-5f8f-4a62-f0ec-53e6fc78c9b7","papermill":{"duration":0.125884,"end_time":"2025-05-27T21:50:00.191231","exception":false,"start_time":"2025-05-27T21:50:00.065347","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"0705900e","cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport logging\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.data import random_split\n# Load utility functions from cloned repository\nfrom src.loadData import GraphDataset\nfrom src.utils import set_seed\nfrom src.models import GNN\nimport argparse\n\n# Set the random seed\nset_seed()\n","metadata":{"id":"lAQuCuIoBbq5","papermill":{"duration":4.580563,"end_time":"2025-05-27T21:50:04.775612","exception":true,"start_time":"2025-05-27T21:50:00.195049","status":"failed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"fb7ccfea","cell_type":"code","source":"def add_zeros(data):\n    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n    return data","metadata":{"id":"Dyf0I2-t9IcW","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"a61b8e4a","cell_type":"code","source":"def train(\n    data_loader,\n    model,\n    optimizer,\n    scheduler,\n    criterion,\n    device,\n    save_checkpoints=False,\n    checkpoint_path=\"ckpt\",\n    current_epoch=0,\n    grad_clip: float = 2.0,\n):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n\n    for data in tqdm(data_loader, desc=\"Train-batches\", unit=\"batch\"):\n        data = data.to(device)\n        optimizer.zero_grad()\n\n        output = model(data)\n        loss   = criterion(output, data.y)\n        loss.backward()\n\n        # Gradient-clipping per stabilità\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n        total   += data.y.size(0)\n\n    scheduler.step()\n\n    # Eventuale checkpoint\n    if save_checkpoints:\n        fname = f\"{checkpoint_path}_epoch_{current_epoch+1}.pth\"\n        torch.save(model.state_dict(), fname)\n        print(f\"✓ Checkpoint salvato: {fname}\")\n\n    return total_loss / len(data_loader), correct / total","metadata":{"id":"3jKvoQYI9Zbc","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"4f1eba09","cell_type":"code","source":"def evaluate(data_loader, model, device, calculate_accuracy=False):\n    model.eval()\n    correct = 0\n    total = 0\n    predictions = []\n    total_loss = 0\n    criterion = torch.nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n            data = data.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            if calculate_accuracy:\n                correct += (pred == data.y).sum().item()\n                total += data.y.size(0)\n                total_loss += criterion(output, data.y).item()\n            else:\n                predictions.extend(pred.cpu().numpy())\n    if calculate_accuracy:\n        accuracy = correct / total\n        return  total_loss / len(data_loader),accuracy\n    return predictions","metadata":{"id":"8peFiIS19ZpK","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"a42febb8","cell_type":"code","source":"def save_predictions(predictions, test_path):\n    script_dir = os.getcwd() \n    submission_folder = os.path.join(script_dir, \"submission\")\n    test_dir_name = os.path.basename(os.path.dirname(test_path))\n    \n    os.makedirs(submission_folder, exist_ok=True)\n    \n    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n    \n    test_graph_ids = list(range(len(predictions)))\n    output_df = pd.DataFrame({\n        \"id\": test_graph_ids,\n        \"pred\": predictions\n    })\n    \n    output_df.to_csv(output_csv_path, index=False)\n    print(f\"Predictions saved to {output_csv_path}\")","metadata":{"id":"WanuZKxy9Zs-","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"ce96b9f4","cell_type":"code","source":"def plot_training_progress(train_losses, train_accuracies, output_dir):\n    epochs = range(1, len(train_losses) + 1)\n    plt.figure(figsize=(12, 6))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Loss per Epoch')\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training Accuracy per Epoch')\n\n    # Save plots in the current directory\n    os.makedirs(output_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n    plt.close()","metadata":{"id":"uyHIJS5U9ZzB","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"4c8aa012","cell_type":"code","source":"def get_user_input(prompt, default=None, required=False, type_cast=str):\n\n    while True:\n        user_input = input(f\"{prompt} [{default}]: \")\n        \n        if user_input == \"\" and required:\n            print(\"This field is required. Please enter a value.\")\n            continue\n        \n        if user_input == \"\" and default is not None:\n            return default\n        \n        if user_input == \"\" and not required:\n            return None\n        \n        try:\n            return type_cast(user_input)\n        except ValueError:\n            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"3cbff940","cell_type":"code","source":"def get_arguments():\n    args = {}\n    #args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n    args['train_path'] = \"/kaggle/working/hackaton/datasets/A/train.json.gz\"\n    #args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n    args['test_path'] = \"/kaggle/working/hackaton/datasets/A/test.json.gz\"\n    #args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n    args['num_checkpoints'] = 3\n    #args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n    args['drop_ratio'] = 0.1\n    #args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n    args['batch_size'] = 64\n    #args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n    args['epochs'] = 10\n    #args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n    #args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n\n    \n    return argparse.Namespace(**args)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d6447b12","cell_type":"code","source":"def populate_args(args):\n    print(\"Arguments received:\")\n    for key, value in vars(args).items():\n        print(f\"{key}: {value}\")\nargs = get_arguments()\npopulate_args(args)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"9a44a887","cell_type":"code","source":"class NoisyCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, p_noisy):\n        super().__init__()\n        self.p = p_noisy\n        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets):\n        losses = self.ce(logits, targets)\n        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n        return (losses * weights).mean()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"b359f73e-8c62-422b-a8fa-3ab3e1651139","cell_type":"code","source":"class SymmetricCrossEntropy(nn.Module):\n    \"\"\"\n    Robust loss (Ghosh et al. 2017) contro label rumorose.\n    L = α · CE(p, q)  +  β · CE(q, p)\n    \"\"\"\n    def __init__(self, num_classes: int, alpha: float = 0.7, beta: float = 1.3):\n        super().__init__()\n        self.alpha = alpha\n        self.beta  = beta\n        self.ce    = nn.CrossEntropyLoss()\n        self.num_classes = num_classes\n\n    def forward(self, logits, target):\n        ce = self.ce(logits, target)\n\n        # Reverse CE  (model distribuz. → label one-hot)\n        pred = torch.softmax(logits, dim=1).clamp(min=1e-7, max=1.0)\n        target_one_hot = torch.zeros_like(pred).scatter_(1, target.view(-1,1), 1)\n        rce = (- (target_one_hot * torch.log(pred)).sum(dim=1)).mean()\n\n        return self.alpha * ce + self.beta * rce","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d9ec06f6-ef6e-47e7-9413-dae80207bb48","cell_type":"code","source":"import torch.nn as nn\nfrom torch_geometric.nn import SAGEConv, JumpingKnowledge, GlobalAttention\nfrom torch_geometric.utils import dropout_edge, add_self_loops\n\nclass GraphSAGEClassifier(nn.Module):\n    \"\"\"\n    3 layer GraphSAGE con:\n      • aggregatore 'max'\n      • BatchNorm + Dropout dopo ogni layer\n      • Jumping-Knowledge ('cat') per evitare oversmoothing\n      • Global Attention Pooling per pesare i nodi rilevanti\n    Usa un embedding fittizio (featureless) come nella tua versione.\n    \"\"\"\n    def __init__(\n        self,\n        input_dim:  int = 128,\n        hidden_dim: int = 128,\n        output_dim: int = 6,\n        num_layers: int = 3,\n        dropout:    float = 0.3,\n        jk_mode:    str   = \"cat\",\n    ):\n        super().__init__()\n\n        # Featureless embedding (1 categoria di nodi)\n        self.embedding = nn.Embedding(1, input_dim)\n\n        # ── GraphSAGE stack ────────────────────────────────────────────────────\n        self.convs, self.bns = nn.ModuleList(), nn.ModuleList()\n        for i in range(num_layers):\n            in_dim = input_dim if i == 0 else hidden_dim\n            self.convs.append(SAGEConv(in_dim, hidden_dim, aggr=\"max\"))\n            self.bns.append(nn.BatchNorm1d(hidden_dim))\n\n        # Jumping-Knowledge per combinare i layer\n        self.jump = JumpingKnowledge(jk_mode)\n        jk_out_dim = hidden_dim * num_layers if jk_mode == \"cat\" else hidden_dim\n\n        # Global Attention pooling (gate = MLP a 2 layer)\n        self.pool = GlobalAttention(\n            gate_nn = nn.Sequential(\n                nn.Linear(jk_out_dim, jk_out_dim // 2),\n                nn.ReLU(),\n                nn.Linear(jk_out_dim // 2, 1)\n            )\n        )\n\n        self.drop = nn.Dropout(dropout)\n        self.fc   = nn.Linear(jk_out_dim, output_dim)\n\n    # ─────────────────────────────────────────────────────────────────────────\n    def forward(self, data):\n        # (1) self-loops & edge-dropout on-the-fly\n        edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n        edge_index, _ = dropout_edge(edge_index, p=0.2, training=self.training)\n\n        # (2) node embedding\n        x = self.embedding(data.x.squeeze())  # (N, input_dim)\n\n        # (3) GraphSAGE layers\n        xs = []\n        for conv, bn in zip(self.convs, self.bns):\n            x = conv(x, edge_index)\n            x = bn(x)\n            x = torch.relu(x)\n            x = self.drop(x)\n            xs.append(x)\n\n        # (4) Jumping-Knowledge aggregation\n        x = self.jump(xs)                     # (N, jk_out_dim)\n\n        # (5) Graph-level representation\n        x = self.pool(x, data.batch)          # (B, jk_out_dim)\n        return self.fc(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a2495881","cell_type":"code","source":"script_dir = os.getcwd() \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n    \nmodel = GraphSAGEClassifier().to(device)\n\noptimizer  = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)\nscheduler  = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=num_epochs, eta_min=1e-5\n)\ncriterion  = SymmetricCrossEntropy(num_classes=6).to(device)","metadata":{"id":"lHX55XGECXBr","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"1f2b6a4e","cell_type":"code","source":"test_dir_name = os.path.basename(os.path.dirname(args.test_path))\nlogs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\nlog_file = os.path.join(logs_folder, \"training.log\")\nos.makedirs(os.path.dirname(log_file), exist_ok=True)\nlogging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\nlogging.getLogger().addHandler(logging.StreamHandler())\n\ncheckpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\ncheckpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\nos.makedirs(checkpoints_folder, exist_ok=True)\n","metadata":{"id":"BTYT5jYuChPb","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"fea26420","cell_type":"code","source":"if os.path.exists(checkpoint_path) and not args.train_path:\n    model.load_state_dict(torch.load(checkpoint_path))\n    print(f\"Loaded best model from {checkpoint_path}\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e89ff574","cell_type":"code","source":"if args.train_path:\n    full_dataset = GraphDataset(args.train_path, transform=add_zeros)\n    val_size = int(0.2 * len(full_dataset))\n    train_size = len(full_dataset) - val_size\n\n    \n    generator = torch.Generator().manual_seed(12)\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n \n    print(num_classes = len(set(d.y.item() for d in train_dataset)))\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n\n    num_epochs = args.epochs\n    best_val_accuracy = 0.0   \n\n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n\n    if num_checkpoints > 1:\n        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n    else:\n        checkpoint_intervals = [num_epochs]\n\n    for epoch in range(num_epochs):\n        train_loss, train_acc = train(\n            train_loader, model, optimizer, criterion, device,\n            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n            current_epoch=epoch\n        )\n\n        val_loss,val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n        train_losses.append(train_loss)\n        train_accuracies.append(train_acc)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n\n        \n        if val_acc > best_val_accuracy:\n            best_val_accuracy = val_acc\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f\"Best model updated and saved at {checkpoint_path}\")\n\n    plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n    plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"f0475a6f","cell_type":"code","source":"import gc\ndel train_dataset\ndel train_loader\ndel full_dataset\ndel val_dataset\ndel val_loader\ngc.collect()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"29376036","cell_type":"code","source":"test_dataset = GraphDataset(args.test_path, transform=add_zeros)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n    ","metadata":{"id":"xsXZIj4Mdu3I","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"ff79562a","cell_type":"code","source":"model.load_state_dict(torch.load(checkpoint_path))\npredictions = evaluate(test_loader, model, device, calculate_accuracy=False)\nsave_predictions(predictions, args.test_path)","metadata":{"id":"x1OnGq_nCmTr","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"deb91128","cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}